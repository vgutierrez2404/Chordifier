{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np  \n",
    "import librosa \n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch  \n",
    "import torch.nn as nn   \n",
    "from typing import List\n",
    "import pytorch_lightning as pl  \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataframe = pd.read_csv('label_dataframe.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir):\n",
    "        self.dataframe = dataframe['file_name']\n",
    "        self.root_dir = root_dir\n",
    "        self.root_note = dataframe['root_note']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure correct indexing\n",
    "        file_name = self.dataframe.iloc[idx]\n",
    "        root_note = self.root_note.iloc[idx]\n",
    "        # Construct the file path correctly\n",
    "        chromagram_path = os.path.join(self.root_dir, file_name.split('.wav')[0] + '_chromagram.npy')\n",
    "        \n",
    "        try:\n",
    "            data = torch.Tensor(np.load(chromagram_path))\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: File not found: {chromagram_path}\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the file: {chromagram_path}\")\n",
    "            raise e\n",
    "        \n",
    "        return data, file_name, root_note\n",
    "    # def __len__(self):\n",
    "    #     return len(self.dataframe)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     # chromagram_path = os.path.join(self.root_dir, self.dataframe.iloc[idx][0].split('.wav')[0] + '_chromagram.npy')\n",
    "    #     # return np.load(chromagram_path)\n",
    "    #     file_name = self.dataframe.iat[idx, 0]\n",
    "    #     chromagram_path = os.path.join(self.root_dir, file_name.split('.wav')[0] + '_chromagram.npy')\n",
    "    #     return np.load(chromagram_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/jdh_sf0s6059j5bw53rnglhr0000gn/T/ipykernel_37785/1261890182.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  chroma_oath = os.path.join('IDMT-SMT-CHORDS/chromagrams', row[0].split('.wav')[0] + '_chromagram.npy')\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['label', 'instrument', 'type_of_sound', 'chord_type']\n",
    "\n",
    "X = audio_dataframe.drop(columns=columns_to_drop, axis=1)\n",
    "y = audio_dataframe['root_note']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "for index, row in X_train.iterrows(): \n",
    "    try:\n",
    "        chroma_oath = os.path.join('IDMT-SMT-CHORDS/chromagrams', row[0].split('.wav')[0] + '_chromagram.npy')\n",
    "        #print(chroma_oath)\n",
    "    except:\n",
    "        print('gasdf') \n",
    "\n",
    "\n",
    "train_dataloader = CustomDataset(dataframe=X_train, root_dir='IDMT-SMT-CHORDS/chromagrams')\n",
    "test_dataloader = CustomDataset(dataframe=X_test, root_dir='IDMT-SMT-CHORDS/chromagrams')\n",
    "val_dataloader = CustomDataset(dataframe=X_val, root_dir='IDMT-SMT-CHORDS/chromagrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, encoded_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Linear(input_dim, h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(h_dim)\n",
    "            ))\n",
    "            input_dim = h_dim\n",
    "        layers.append(nn.Linear(input_dim, encoded_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoded_dim, hidden_dims, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        layers = []\n",
    "        hidden_dims.reverse()\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Linear(encoded_dim, h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(h_dim)\n",
    "            ))\n",
    "            encoded_dim = h_dim\n",
    "        layers.append(nn.Linear(encoded_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, input_dim=12, hidden_dims=None, encoded_dim=12, learning_rate=1e-4):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [128, 64, 32]\n",
    "        self.encoder = Encoder(input_dim, hidden_dims, encoded_dim)\n",
    "        self.decoder = Decoder(encoded_dim, hidden_dims, input_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, _, _ = batch\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = nn.MSELoss()(outputs, inputs)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, _, _ = batch\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = nn.MSELoss()(outputs, inputs)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def encode(self, x): \n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 12  # For MNIST dataset\n",
    "hidden_dims = [128, 64, 32]\n",
    "encoded_dim = 12\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "columns_to_drop = ['label', 'instrument', 'type_of_sound', 'chord_type']\n",
    "\n",
    "X = audio_dataframe.drop(columns=columns_to_drop, axis=1)\n",
    "y = audio_dataframe['root_note']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(dataframe=X_train, root_dir='IDMT-SMT-CHORDS/chromagrams')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = CustomDataset(dataframe=X_test, root_dir='IDMT-SMT-CHORDS/chromagrams')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = CustomDataset(dataframe=X_val, root_dir='IDMT-SMT-CHORDS/chromagrams')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "full_dataset= CustomDataset(dataframe=X, root_dir='IDMT-SMT-CHORDS/chromagrams')\n",
    "full_dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 12.8 K\n",
      "1 | decoder | Decoder | 12.8 K\n",
      "------------------------------------\n",
      "25.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.7 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf038543a94e45a98569dae66e77e628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2ecc8b8a814bd29ca03e2d4722f04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8748f1e0bbee40cb920e10872fd5095a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9552e2d39642d992ab6201a7a18dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bde05d33b44a9b86d6d9c8f994c6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c31dd5e6884c2ca40434303056fc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d338d179b0684290a3211850fcdfdd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08edb523dd9a4b8c84e21b64800620fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77889418216c41b7a64039f23dcbf077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dffaac8e2643cda21df2e9c1c7e5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b5d60fbd3046e3a0ae37f2a0df33da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d36a609142433184228dadee1e5439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autoencoder = Autoencoder(input_dim, hidden_dims, encoded_dim, learning_rate)\n",
    "\n",
    "# Training\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "trainer.fit(autoencoder, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file_name root_note  encoded_0  encoded_1  encoded_2  encoded_3  encoded_4  \\\n",
      "0  0000.wav         C   0.618062   0.891722   0.282112   0.146800   0.478934   \n",
      "1  0001.wav         C  -0.322870   0.314766   0.694106  -0.305864  -0.163472   \n",
      "2  0002.wav         C   0.635033   0.434493   1.508799  -0.677667   0.232426   \n",
      "3  0003.wav        C#  -0.569648  -0.249012   0.913353   0.658161   1.618873   \n",
      "4  0004.wav        C#   0.937316   0.078704   3.566514  -0.076888   2.564118   \n",
      "\n",
      "   encoded_5  encoded_6  encoded_7  encoded_8  encoded_9  encoded_10  \\\n",
      "0  -1.229736   0.094583  -0.302654  -1.085481  -1.637345    0.959388   \n",
      "1   0.502251  -0.084436  -0.158671  -0.008823  -0.672769    0.965743   \n",
      "2  -1.076412   0.082173  -0.665262  -0.818527  -1.500026    0.785322   \n",
      "3   0.392390   0.976987  -0.953056   0.643535   0.577639   -0.267624   \n",
      "4   1.069315  -0.471510   0.013139  -0.240945   2.792057   -1.551140   \n",
      "\n",
      "   encoded_11  \n",
      "0   -0.334417  \n",
      "1   -0.136161  \n",
      "2   -1.133469  \n",
      "3   -0.066476  \n",
      "4   -0.317655  \n"
     ]
    }
   ],
   "source": [
    "# Inference to get encoded data\n",
    "autoencoder.eval()  # Set model to evaluation mode\n",
    "\n",
    "encoded_data_list = []\n",
    "file_names_list = []\n",
    "root_note_list = []\n",
    "# Iterate over DataLoader\n",
    "for batch in full_dataloader:\n",
    "    batch_data, file_names, root_notes = batch\n",
    "    batch_data = batch_data.view(batch_data.size(0), -1).float()\n",
    "    with torch.no_grad():\n",
    "        encoded_batch = autoencoder.encode(batch_data)\n",
    "    encoded_data_list.append(encoded_batch.cpu().numpy())\n",
    "    file_names_list.extend(file_names)\n",
    "    root_note_list.extend(root_notes)\n",
    "# Concatenate all encoded data\n",
    "encoded_data = np.concatenate(encoded_data_list, axis=0)\n",
    "\n",
    "# Save to DataFrame with filenames\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=[f'encoded_{i}' for i in range(encoded_dim)])\n",
    "encoded_df.insert(0, 'file_name', file_names_list)\n",
    "encoded_df.insert(1, 'root_note', root_note_list)\n",
    "print(encoded_df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "encoded_df.to_csv('encoded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Encoding\n",
    "# encoded_data = autoencoder.encoder(sample_data)\n",
    "# print(\"Encoded data:\", encoded_data)\n",
    "\n",
    "# # Decoding\n",
    "# decoded_data = autoencoder.decoder(encoded_data)\n",
    "# print(\"Decoded data:\", decoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the weights of the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), 'autoencoder.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
